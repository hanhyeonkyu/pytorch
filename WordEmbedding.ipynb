{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "479e9032-0ffb-4e9a-b5e8-71da0bfbaf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# word to sparse, dense representation\n",
    "import torch\n",
    "# one-hot vector -> almost value is 0 -> sparse representation\n",
    "dog = torch.FloatTensor([1, 0, 0, 0, 0])\n",
    "cat = torch.FloatTensor([0, 1, 0, 0, 0])\n",
    "computer = torch.FloatTensor([0, 0, 1, 0, 0])\n",
    "netbook = torch.FloatTensor([0, 0, 0, 1, 0])\n",
    "book = torch.FloatTensor([0, 0, 0, 0, 1])\n",
    "# vector similarity\n",
    "print(torch.cosine_similarity(dog, cat, dim=0))\n",
    "print(torch.cosine_similarity(cat, computer, dim=0))\n",
    "print(torch.cosine_similarity(computer, netbook, dim=0))\n",
    "print(torch.cosine_similarity(netbook, book, dim=0))\n",
    "# sparse representation have disadvantage.\n",
    "# because if word increase, vector demenstion increse continuously. and can not apply similarity.\n",
    "# dense representation can determine vector demensions. -> it's word embedding.\n",
    "# word embedding way -> LSA, word2vec, fasttext, glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae84a90c-528a-4ed2-b9fb-dbf4b18a99da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word 2 Vec -> CBOW(Continuous Bag of Words), Skip-Gram\n",
    "# distributed representation with distributional hypothesis.\n",
    "# hypothesis is similar position, similar meaning. -> NNLM, RNNLM, Word2Vec\n",
    "# CBOW -> predict middle words with near words.\n",
    "# Skip-gram -> predict near words with middle words.\n",
    "# efficiency - CBOW < Skip-gram\n",
    "# Skip-gram with Negative Sampling way is more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77341a62-e55f-4e9f-8cb6-f9f44ee168a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GloVe(Global Vectors for Word Representation) -> use both count base and predict base\n",
    "# # LSA - count base, Word2Vec - predict base\n",
    "# from glove import Corpus, Glove\n",
    "\n",
    "# corpus = Corpus() \n",
    "# corpus.fit(result, window=5)\n",
    "# # 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성\n",
    "\n",
    "# glove = Glove(no_components=100, learning_rate=0.05)\n",
    "# glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
    "# glove.add_dictionary(corpus.dictionary)\n",
    "# # 학습에 이용할 쓰레드의 개수는 4로 설정, 에포크는 20.\n",
    "# model_result1=glove.most_similar(\"man\")\n",
    "# print(model_result1)\n",
    "# model_result2=glove.most_similar(\"boy\")\n",
    "# print(model_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c24e21c-5f5f-4984-9ac6-3b1504518137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0752,  0.7725,  0.9024],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.0689, -1.5479, -0.8607],\n",
      "        [-1.2232, -0.0610, -1.6972],\n",
      "        [-1.6320,  0.1375,  0.2597],\n",
      "        [-0.7926, -0.2789, -0.4497],\n",
      "        [-0.1404,  0.9512, -0.8819],\n",
      "        [-0.0345,  0.3402, -1.6523]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "train_data = 'you need to know how to code'\n",
    "word_set = set(train_data.split()) # 중복을 제거한 단어들의 집합인 단어 집합 생성.\n",
    "vocab = {tkn: i+2 for i, tkn in enumerate(word_set)}  # 단어 집합의 각 단어에 고유한 정수 맵핑.\n",
    "vocab['<unk>'] = 0\n",
    "vocab['<pad>'] = 1\n",
    "import torch.nn as nn\n",
    "embedding_layer = nn.Embedding(num_embeddings = len(vocab), \n",
    "                               embedding_dim = 3,\n",
    "                               padding_idx = 1)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d33a3d31-430e-4124-b98d-3d488df357c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 84.1M/84.1M [00:27<00:00, 3.05MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기 : 25000\n",
      "{'text': ['for', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'a', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem.', 'imagine', 'a', 'movie', 'where', 'joe', 'piscopo', 'is', 'actually', 'funny!', 'maureen', 'stapleton', 'is', 'a', 'scene', 'stealer.', 'the', 'moroni', 'character', 'is', 'an', 'absolute', 'scream.', 'watch', 'for', 'alan', '\"the', 'skipper\"', 'hale', 'jr.', 'as', 'a', 'police', 'sgt.'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "from torchtext.legacy import data, datasets\n",
    "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
    "LABEL = data.Field(sequential=False, batch_first=True)\n",
    "trainset, testset = datasets.IMDB.splits(TEXT, LABEL)\n",
    "print('훈련 데이터의 크기 : {}' .format(len(trainset)))\n",
    "print(vars(trainset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b89c2227-8500-4561-aaf1-60ae90f9574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "# word2vec_model = KeyedVectors.load_word2vec_format('eng_w2v')\n",
    "# print(word2vec_model['this']) # 영어 단어 'this'의 임베딩 벡터값 출력\n",
    "# print(word2vec_model['self-indulgent']) # 영어 단어 'self-indulgent'의 임베딩 벡터값 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ca95ccf-78cd-421a-87ae-2bf71a3ea795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torchtext.vocab import Vectors\n",
    "# vectors = Vectors(name=\"eng_w2v\") # 사전 훈련된 Word2Vec 모델을 vectors에 저장\n",
    "# TEXT.build_vocab(trainset, vectors=vectors, max_size=10000, min_freq=10) # Word2Vec 모델을 임베딩 벡터값으로 초기화\n",
    "# print(TEXT.vocab.stoi)\n",
    "# print('임베딩 벡터의 개수와 차원 : {} '.format(TEXT.vocab.vectors.shape))\n",
    "# print(TEXT.vocab.vectors[0]) # <unk>의 임베딩 벡터값\n",
    "# print(TEXT.vocab.vectors[1]) # <pad>의 임베딩 벡터값\n",
    "# print(TEXT.vocab.vectors[10000]) # 단어 'self-indulgent'의 임베딩 벡터값\n",
    "# embedding_layer = nn.Embedding.from_pretrained(TEXT.vocab.vectors, freeze=False)\n",
    "# print(embedding_layer(torch.LongTensor([10]))) # 단어 this의 임베딩 벡터값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b60a7a-2c8a-4784-90ac-8f7858ee2c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
